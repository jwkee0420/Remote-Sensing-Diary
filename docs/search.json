[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jin’s Learning Diary CASA0023",
    "section": "",
    "text": "CASA0023 Jin’s Learning Diary\nWelcome to my learning diary space which records learnings, reflections and musings from the teaching at CASA0023 Remote Sensing Cities and Environment as part of my studies at UCL’s MSc Urban Spatial Science.\nPrior to my studies at UCL, I was enrolled at the Bartlett School of Planning BSc Urban Planning, Design and Management programme. Hence, my background lies in more ‘humanities’ aligned topics like urban design, policy and sustainable development.\nIf we go even further back, I do have some background in more technical fields such as GIS and remote sensing. I served as a geospatial analyst in the Singapore Armed Forces during my 2-year military conscription prior to my undergraduate studies where I was taught softwares like ArcGIS. I later interned at the Housing and Development Board (HDB) and used Landsat-8/MODIS data to study Urban Heat Island effect in Singapore towns.\nMy interest in the built environment thus extends to the concept of liveability - how do we measure such a subjective term which can incorporate both technical (e.g temperature, shade, amenities) and design aspects (sense of safety, perceptions and emotions from facades).\nAcknowledgements:\nThis site is built using Quarto.",
    "crumbs": [
      "CASA0023 Jin's Learning Diary"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Getting started with Remote Sensing",
    "section": "",
    "text": "1.1 Introduction\nThe first week of Remote Sensing covers the definitions of remote sensing, what we can do with remote sensing and the technology behind it. The basic understanding of remote sensing is a definition by NASA: “acquiring information from a distance” which is quite broad. This means that even taking a photograph using mobile phones is considered remote sensing. However, the course is centred around satellite images and data, which covers a much larger area in specific areas of the earth and there are a wide range of tools, methods and data sources to choose from when analysing satellite data.\nFor example, there are many different sources of open satellite data that is available for analysis such as MODIS from NASA, Landsat-8 from USGS, and Sentinel-2 from the European Space Agency. They all capture data at different resolutions and capture different data types based on their sensor technologies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with Remote Sensing</span>"
    ]
  },
  {
    "objectID": "chapter1.html#introduction",
    "href": "chapter1.html#introduction",
    "title": "1  Getting started with Remote Sensing",
    "section": "",
    "text": "Landsat-8 with its different sensors and resolutions. Source: NASA\n\n\n1.1.1 Sensors\nTwo broad categories of sensors are active sensors and passive sensors. Passive sensors rely on reflect energy from the sun and thus needs to operate in the day, while active sensors can emit electromagnetic waves, and hence can be operated in the night, giving unique products like Nighttime satellite imagery. Synthetic Aperture Radar, a type of passive sensor, is also known for its all-weather penetration ability and thus is suitable for continuous monitoring. A lot of post-processing of the data also occurs behind the instruments; satellite data download typically has atmospheric corrections applied to it (e.g Landsat Level 2 surface reflectance rather than TOA) so that the data quality is enhanced and suited for scientific analysis.\nThe practical covers the comparison between two sources of satellite data – Sentinel-2 and Landsat-8. They contain different products which determines the different resolutions in terms of spatial, spectral, temporal and radiometric.\nThe city of choice used in this practical is Kanagawa, Japan. It sits south of the Tokyo metropolitan ward, and has a diverse landuse cover ranging from heavy industries near the sea, to residential and the mountainous region to the west. The different band combinations of the satellite data can be used for various analytical functions, such as to identify a specific land use or soil health for example.\n\n False colour composite of Kanagawa which helps identifying greenery, soil and water bodies. Image produced with data from Landsat-8, processed in SNAP.\n\nAnother key concept is spectral signatures. Since different objects on the ground reflect electromagnetic waves differently across different wavelengths, we can capture the reflectance and associate it with a particular land use. Green vegetation absorbs red wavelengths but reflect NIR waves for example.\n Spectral bands of various land use covers. Image produced with data from Landsat-8, processed in RStudio.\nThe graph above is obtained from identifying (visually) various land use cover by drawing boundaries around them, and extracting the range of spectral signatures across the 6 bands. But this is just 5 land cover types. I presume the accuracy and box plot for each band will be smaller with more data put into identifying land uses since I did a pretty quick identification visually.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with Remote Sensing</span>"
    ]
  },
  {
    "objectID": "chapter1.html#applications",
    "href": "chapter1.html#applications",
    "title": "1  Getting started with Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nA systematic review of remote sensing literature using Landsat-8 data was published by Hemati et al. (2021) which covers the trends, opportunities and future of using Landsat-8 data for various studies. The key takeaway is that the single most important feature of remote sensing is land use change, which 60% are attributed to human activities like industrialisation and urbanisation. Monitoring land use changes can yield important insights into how these processes are unfolding. The advancement of cloud computing platforms such as Google Earth Engine makes analysing satellite data over large areas across time possible and much easier than ever before. This was seen in how the number of images used per articles skyrocketed since 2000s with increased computation power. The most common land use change detection carried out is deforestation and urban expansion, and many data sources are often fused to improve data resolution of various products. Hemati et. al (2021) reports that Sentinel-2 and MODIS data are often fused with Landsat-8, thereby reaping benefits of different data sources – Sentinel for better spatial resolution and Landsat-8 for historical continuity for example.\nAn example of this is an algorithm developed by Gao et. al (2006) which fuses MODIS and Landsat-8 data to overcome limitations which previously hindered researchers from studying temporal patterns of plant and farming cycle/patterns. Of course, these applications are just merely scratching the surface of what remote sensing data can achieve. However, the worsening climate change and associated extreme climate events mean that remote sensing is becoming more important than ever before which can help understand and provide potential solutions and management tools.\n\n\n\nAs of the time of writing, the 2025 LA wildfires continues to blaze on after more than two weeks. Remote sensing is a useful tool to monitor the spread of the disaster and alert civilians to evacuate. Source: New York Times",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with Remote Sensing</span>"
    ]
  },
  {
    "objectID": "chapter1.html#reflections",
    "href": "chapter1.html#reflections",
    "title": "1  Getting started with Remote Sensing",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nI previously worked used satellite data during my internship in the Housing Development Board in Singapore just before entering university. There are some quite real implications for policies and uses of satellite data in Singapore just because it built so densely. I was tasked with building a model to predict Land Surface Temperature, using factors such as land use classes, sky-view factors and impervious surface area.\nDue to the sheer density of the built environment in Singapore coupled with climate change, thermal comfort becomes a large problem living in Singapore and urban planners are looking into using satellite data to help inform the factors which affect land surface temperatures. With remote sensing, planners can better understand the land use mix, configurations of tall buildings, and other elements which affects the built environment layout that can minimise heat retention in urban settings. The challenge however, is collaborating with planners, architects, transport operators, and a diverse range of stakeholders before reaching a consensus of how to plan cities. These stakeholders might not understand what remote sensing is or how it contributes to better planning, hence effective communication and translation of remote sensing data into actionables (policy, action plans etc) are crucial.\nFinally, although I worked with these remote sensing data before, I did not know the technical index of the different datasets and even the sensors of the satellites. I am glad that learning remote sensing again ‘from scratch’ enabled me to really examine how the data is captured and processed in depth, and appreciate the calibrations and issues with mapping the Earth. Spatial data scientists have to understand the technical specifications of different datasets, balance the pros and cons, and make a decision on which datasets to use for their research. I hope that through this module, I can think more like a spatial data scientist who critically evaluates data sources and methodologies, considers the limitations and strengths of different remote sensing technologies, and applies this knowledge to solve real-world problems effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with Remote Sensing</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  The essential tools: Xaringan & Quarto",
    "section": "",
    "text": "To view this presentation full-screen, click here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The essential tools: Xaringan & Quarto</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  All about Remote Sensing Data",
    "section": "",
    "text": "3.1 Pre-processing\nThis week’s content is slightly more technical, covering the different flaws in remote-sensing data and the different ways we can account and correct for them. The flaws and artifacts in the data can come from the technical properties of the sensor, to atmospheric conditions while capturing data across earth. Each flaw is a huge can of worms, with extensive research and modelling happening in the background to ensure that the data that we download publicly is fit to use without distortions or influences from weather, sensor flaws or orthographic projection errors.\nIn simple terms, the pre-processing types can be arranged into four seperate categories:\nI also created a small dictionary for key terms learnt this week for quick referencing and look up of definitions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>All about Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "chapter3.html#pre-processing",
    "href": "chapter3.html#pre-processing",
    "title": "3  All about Remote Sensing Data",
    "section": "",
    "text": "Type of Correction\nImage Flaw and Causes\nExample\n\n\n\n\nGeometric Correction\nImage distortion introduced from view angle, topography, wind conditions and rotation of the earth. Using data collected from the sensor such as exact orbit, altitude, imaging parameters, and ground control points, images are resampled so that locations and accurate pixel values can be calculated.\n\nImage credits: Bastorous (2020)\nResampling methods llike nearest neighbour, bilinear and bicubic (illustrated) are a key geometric correction step when reprojecting different geographic referencing systems which are found in many GIS softwares today.\n\n\nAtmospheric Correction (subset of radiometric correction)\nCaused by atmospheric scattering of light which can create ‘adjacency effect’. Atmospheric correction is also divided into two methods: absolute and relative.\n\nImage Credits: Sun et. al (2017)\nAbsolute atmospheric correction examples:\n\nUsing atmospheric radiative transfer models which converts digital numbers (DN) into scaled surface reflectance. This requires either private tools (which costs money) or open-source tools\nEmpirical Line Correction using in-situ reflectance measurements of a known material and establishing a linear model\n\nRelative atmospheric corrections examples:\n\nDark Object Subtraction (DOS) correct images relative to a single image or other images, using a dark object feature (like water) as a reference point, assuming near zero reflectance.\n\n\n\nOrthorectification Correction (subset of geometric correction)\nDistortions caused by sensor tilt and topographical relief. Orthorectification ensures that each point appear as if each pixel is captured directly below the sensor.\nAn example orthorectification correction is cosine correction which accounts for the angle at which sunlight hits the earth, causing variability in brightness.\n\n\nRadiometric Correction\nSpectral information captured by satellites are affected by atmospheric scattering, absorption which require correction, turning DNs to spectral radiance values.\nThe data handbook for Landsat-8 outlines several radiometric corrections based on the sensor property in order to correct for inconsistency for specific spectral bands. These include the OLI solar diffuser, lunar irradiance and underfly acquisitions among many others.\n\n\n\n\n\n\n\nTerminology\nDefinition\n\n\n\n\nDigital Number (DN)\n\nNumerical values that represent the intensity of electromagnetic radiation and usually converted to radiance/reflectance\nRaw data collected by satellites and range from 0-65535 (Landsat-8)\nUnitless - do not represent a physical measurement\n\n\n\nRadiance\n\nAlso called Top-of-Atmosphere (TOA) radiance\nIncludes radiation from sun, neighbouring pixels, from clouds etc\nDepends on orientation, path of light, position of target\n\n\n\nReflectance\n\nProperty of the material (which is what we want) extracted from radiant corrected data\nEither TOA reflectance or surface reflectance\n\n\n\nAnalysis Ready Data (ARD)\n\nData which has been pre-processed according to the content above; also include cloud masking\n\n\n\nBand Ratioing\n\nTechnique used to display spectral variations, enhancing contrast by dividing reflectance of one image band by another image band\n\n\n\nSpectral Indices\n\nAre essentially band ratios or a combination of band ratios\nReduce impact of atmospheric conditions or illumination differences\nA library of indices can be found here\n\n\n\nHigh/Low Pass Filter\n\nImage processing techniques\nLow pass = smoothing, reduce edges/noise\nHigh pass = sharpen, enhance edge and fine details\n\n\n\nTexture\n\nSpatial arrangement of pixels in an image; provides information on surface roughness, patterns which helps determine land use types\nConsider an example like grasslands vs forests: similar spectral properties but different textures\nFirst-order - based on individual pixel values; Second-order - based on spatial arrangement of pixels pairs as seen in GLCM\n\n\n\nPan-sharpen\n\nUsing a higher resolution spectral band to enhance resolution of a lower resolution spectral band\n\n\n\nPrinciple Component Analysis\n\nA process that turns original pixel values into uncorrelated values called Principle Components\nPixel values are standardised before eigenvector/values is computed from a covariance matrix; each pixel is then mapped onto a new PCA axes\nRemoves redundancy, and correlation between bands",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>All about Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "chapter3.html#applications",
    "href": "chapter3.html#applications",
    "title": "3  All about Remote Sensing Data",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nThis week I challenged myself to reading a few more technical papers discussing more about Land Cover and Land-Use (LCLU) applications of remote sensing, with the focus on methodologies used to classify them from Grey-Level Co-occurrence Matrix (GLCM) to machine learning methods such as Random Forest (RF), Support Vector Machine (SVM) in Object Based Image Analysis (OBIA). While it took a long time to understand what the papers were discussing or proposing, I felt that with the help of this week’s content and some googling, I could make sense of what the authors are investigating.\nThe Gray-Level Co-occurrence Matrix (GLCM) is a widely used method for texture analysis, offering valuable insights into land cover classification by capturing spatial relationships between pixel intensities. However, one of the key challenges with GLCM is determining which texture measures and window sizes to use, as these choices can significantly impact the results. Principal Component Analysis (PCA) is often employed to reduce dimensionality and select relevant components, but it requires subjective decisions by the researcher, which can introduce variability in interpretation and outcomes.\nTo address these challenges, Hall-Beyer (2017) explored guidelines for selecting relevant GLCM texture measures across different land covers and window sizes. Her findings suggest that certain texture measures tend to be more effective for classification across diverse land covers. For instance, GLCM Mean is generally a reliable first choice for classification, with GLCM Correlation (Cor) as a useful addition if needed. GLCM Contrast (Con) is particularly effective for identifying edge-like features, while GLCM Entropy (Ent) can enhance classification in more detailed studies.\nHowever, these recommendations are not absolute, as texture measures should be chosen based on the specific characteristics of the image and the features being analyzed. For example, Murray et al. (2010) demonstrated that a combination of Mean, Con, and Ent produced the most accurate land-use maps for tundra vegetation. Hall-Beyer (2017) also emphasized the importance of documenting the chosen texture combinations and their associated accuracies, which can contribute to building a more evidence-based understanding of optimal GLCM settings for different applications. This can help identify evidence-drive patterns and build a library of ‘textures combinations’ which is helpful for all researchers.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>All about Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "chapter3.html#reflections",
    "href": "chapter3.html#reflections",
    "title": "3  All about Remote Sensing Data",
    "section": "3.4 Reflections",
    "text": "3.4 Reflections\nThis week’s content is pretty daunting at first glance, as we covered a lot of technical content behind how pre-processing works (and the math involved!), even though we probably do not need to handle much of the pre-processing ourselves. However, I do agree with Andy that it is still important for us to understand the calibrations and corrections that are occurring behind the scenes so that we understand where our data came from.\nThe second part of the lecture is very interesting as image enhancement is a constantly evolving field, with different researchers developing new techniques or combining old ones to map out features that they would like to investigate. While reading the papers that utilises image enhancement and correction techniques, I learnt a lot more on the intricacies and problems behind mapping each feature, for example issues with mapping vegetation and their individual species. I realised that there is no one size fit all solution to image classification/enhancement; it depends on our own study area, climate and subject that we want to capture/map. All these tools and techniques are available to us but choosing the most appropriate ones and justifying these choices are the most important and crucial part of the research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>All about Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "4  Earth Oberservation and Implications on Policy",
    "section": "",
    "text": "4.1 Case Study: The Singapore Green Plan 2030\nThis week of Remote Sensing covers the idea of how remote sensing can be used to inform policies such as urban planning, resource allocation, or disaster mitigation/adaptation. One important lesson that I took away from the session is how Andy mentioned that many of the remote sensing papers did all the complicated analysis and data wrangling, but an equally important section on policy implications is missing. Linking results from analysis to real world implications such as where initiatives should be targetting or how resources should be allocated is a crucial part of a study, not just identifying challenges and phenomenon using remote sensing.\nThe case study I chose this week is Singapore’s Green Plan 2030. It is the country’s national sustainability movement, aimed to advance climate targets and achieve net zero emissions by 2050. The country currently faces several climate related issues including rising sea levels threatening coasts, rising temperatures in already hot and humid tropical climate, and increased green house gas emissions. The green plan is a joint initiative by five different ministries aimed to address these issues, achieve goals set in the 2016 Paris Agreement of net zero emissions by 2050.\nThe goals and targets are categorised into 5 pillars as show below:\nSource: Singapore Green Plan 2050 Targets.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Earth Oberservation and Implications on Policy</span>"
    ]
  },
  {
    "objectID": "chapter4.html#case-study-the-singapore-green-plan-2030",
    "href": "chapter4.html#case-study-the-singapore-green-plan-2030",
    "title": "4  Earth Oberservation and Implications on Policy",
    "section": "",
    "text": "Pillar\nExample Goals\n\n\n\n\nCity in Nature\n\nDevelop over 130 ha of new parks, and enhance around 170 ha of existing parks with more lush vegetation and natural landscapes\nDouble our annual tree planting rate between 2020 and 2030, to plant 1 million more trees across Singapore\n\n\n\nSustainable Living\n\nGreen Commutes\n\nAchieve 75% mass public transport (i.e. rail and bus) peak-period modal share\nExpand cycling path networks to around 1,300km\n\n\n\n\nEnergy Reset\n\nGreener Infrastructure and Buildings\n\nGreen 80% of Singapore’s buildings (by Gross Floor Area) by 2030\n80% of new buildings (by Gross Floor Area) to be Super Low Energy buildings from 2030\n\n\n\n\nGreen Economy\n\nSustainability as a New Engine for Jobs and Growth\n\nSingapore as a carbon services and trading hub in Asia\nGroom a strong pool of local enterprises to capture sustainability opportunities\n\n\n\n\nResilient Future\n\nAdapt to Sea-level Rise and Enhance Flood Resilience",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Earth Oberservation and Implications on Policy</span>"
    ]
  },
  {
    "objectID": "chapter4.html#applications",
    "href": "chapter4.html#applications",
    "title": "4  Earth Oberservation and Implications on Policy",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nCity in Nature\nThere are many goals relating to increasing areas of green space including parks and planting more trees, but the key question is where should new green spaces be placed across the island state? Remote sensing can potentially provide an accurate assessment of Land-Use Land Cover (LULC) classification which can help evaluate the locations where urban green space is required. By analyzing LULC data, planners can evaluate existing urban landscapes, identify areas lacking green spaces, and prioritize locations where additional parks or tree planting would have the most impact. For example, Bai et al. (2022) developed an urban green space planning evaluation tool based on ecosystem services provision from Land-Use Land Cover to guide the new development of Fengdong New City, balancing residents’ outdoor recreation needs with the benefits of ecosystem services including rivers, forests and greenspace. Using the tool, an ecological corridor was suggested along key ecological nodes within the new town. Similarly, using LULC data to balance new housing developments and retaining green spaces can be a massive advancement towards a sustainable neighbourhood.\nEnergy Reset\nThe high demand of energy use in Singapore’s buildings is also an urgent issue that requires different innovation and technologies to solve. Due to its tropical climate and humidity, coupled with effects of Urban Heat Island (UHI), buildings require large amounts of energy to cool residents and the inhabitants. Reducing energy use through various solutions such as green roofs, solar panels, or even reflective paints can help reduce ambient temperatures and increase thermal comfort for residents. Remote sensing can help assess suitability for green roof and solar panels installations, or identify areas suffering from extreme heat risks, and help deploy more resources to actively cool buildings down, thereby reducing energy use in buildings. Karteris et al. (2016) used very high resolution satellite images to assess green roof potential in Thessaloniki, Northern Greece, and modelled the benefits of a wide area green roof implementation plan across the entire city. The authors found that green roof implementation could lead to a reduction of cooling consumption by 16% and increase rainwater retention by 45%.\n\n Mean percentage available roof for green roof installation obtained from analysis of satellite imagery. Source: Karteris et al. (2016)\n\nThe example demonstrates how remote sensing enables large-scale assessments of green roof feasibility, taking into account factors such as building characteristics (roof slope etc.) , thermal hotspots, and potential cost savings. By providing a data-driven approach, remote sensing can provide a feasible and realistic green roof implementation plan targeting areas where cost-savings can be the highest - all while using open-source data and not expending manpower/labour to check individual roofs for feasibility.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Earth Oberservation and Implications on Policy</span>"
    ]
  },
  {
    "objectID": "chapter4.html#reflections",
    "href": "chapter4.html#reflections",
    "title": "4  Earth Oberservation and Implications on Policy",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nSingapore has a highly ambitious plan which aims to achieve coverage over a range of climate and sustainable development agendas from waste reduction to protection of the coast lines. Although these goals and policies are publicly available online, there is a lack of information of how these goals are going to be carried out, including locations of how certain policies is carried out. Remote sensing could provide data-driven tools to analyse the most suitable locations where schemes can be implemented.\nFortunately, some of the applications of remote sensing has already been rolled out in Singapore. Housing Development Board (HDB), a government board under the Ministry of National Development (MND) has recently announced that heat-resistant paint will be rolled out to all HDB estates following a 2021 pilot study in Tampines, a resident town in East Singapore. Tampines was selected as “HDB’s analysis of satellite images …. showed that the town has a higher average land surface temperature compared to other towns” (HDB, 2021). The new paint is expected to reduce ambient temperatures up to 2 deg C, which is a major improvement to thermal comfort.\n\n Very recent rollout of heat-resistant paint in HDB housing estates. Source: The Strait Times, 2025]\n\nFinally, my final note for the Singapore Green Plan 2030 is that although it is jointly delivered by 5 different ministries, the work seems to be very isolated within each ministry and individual goals are delivered in silo. Even within each ministry, different organisations are working on individual sustainable goals - for example National Parks is in charge of all urban greening projects/schemes, including parks planning and tree planting. On the other hand, waste reduction is solely delivered by the Ministry of Sustainability and Environment.\nHowever, climate and sustainability issues are often interdisciplinary in nature; they require cross collaborations between ministries and a comprehensive plan tackling the problem from different angles. Remote sensing may a potential tool and platform where open source information and data can be shared across multiple ministries, enhancing collaborations and increasing transparency of policy implementation as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Earth Oberservation and Implications on Policy</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "chapter1.html#references",
    "href": "chapter1.html#references",
    "title": "1  Getting started with Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\nGao, F., Masek, J., Schwaller, M. and Hall, F., 2006. On the blending of the Landsat and MODIS surface reflectance: Predicting daily Landsat surface reflectance. IEEE Transactions on Geoscience and Remote sensing, 44(8), pp.2207-2218.\nHemati, M., Hasanlou, M., Mahdianpari, M. and Mohammadimanesh, F., 2021. A systematic review of landsat data for change detection applications: 50 years of monitoring the earth. Remote sensing, 13(15), p.2869.\nNASA, n.d. Landsat 8. Obtained from https://landsat.gsfc.nasa.gov/satellites/landsat-8/\nNew York Times, 2025. Tens of Thousands Told to Evacuate as New Fires Rages North of L.A. Obtained from https://www.nytimes.com/live/2025/01/22/us/los-angeles-wildfires-california",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started with Remote Sensing</span>"
    ]
  },
  {
    "objectID": "chapter3.html#applications-ii",
    "href": "chapter3.html#applications-ii",
    "title": "3  All about Remote Sensing Data",
    "section": "3.3 Applications II",
    "text": "3.3 Applications II\nAfter understanding what GLCM does, I wonder how this fits into a remote sensing data pipeline and how would this be used in a research project? Reading Mohammadpour et al. (2022)’s paper on the use of vegetation indices and textures on vegetation species classification gave me a better understanding of this technique in a research, which is succinctly placed together in the diagram below.\n\n\n\nProcess of vegetation mapping with Sentinel 2. Image taken from Mohammadpour et al. (2022)\n\n\nI think this diagram really aptly summarises the content I learnt this week - first we have reprojection/resampling, which is part of geometric corrections and image enhancement. Clipping region of interest is the same process of this week’s practical, for example using terra::crop and terra::mask. The authors used 4 vegetation indices: Normalized Difference Vegetation Index (NDVI), Green Normalized Difference Vegetation Index (GNDVI), Enhanced Vegetation Index (EVI), and Soil Adjusted Vegetation Index (SAVI), which are various band ratios covered in this week’s lectures as well. Texture extraction using GLCM was seen in the practical, although the paper covered the process in more details; including explaining the choice of texture metrics to bring forward to the Random Forest (RF). The model achieved a high Overall Accuracy of 90.9% and texture metrics seem to have increased the classification accuracy.\nBeyond the high accuracy rate of 90.9%, the paper highlights the importance of complex land classification especially in Portugal where the study is located due to risk of wildfires in the summer. Knowing the locations of different species cluster can help support wildfire management efforts and forest resource management plans.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>All about Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "chapter3.html#references",
    "href": "chapter3.html#references",
    "title": "3  All about Remote Sensing Data",
    "section": "3.5 References",
    "text": "3.5 References\nBastorous, E.B.N., 2020. ROAD NETWORK EXTRACTION FROM SATELLITE IMAGES IN EGYPT. Egypt: Doctoral dissertation, Assiut University.\nHall-Beyer, M., 2017. Practical guidelines for choosing GLCM textures to use in landscape classification tasks over a range of moderate spatial scales. International Journal of Remote Sensing, 38(5), pp.1312-1338.\nMohammadpour, P., Viegas, D.X. and Viegas, C., 2022. Vegetation mapping with random forest using sentinel 2 and GLCM texture feature—A case study for Lousã region, Portugal. Remote Sensing, 14(18), p.4585.\nMurray, H., Lucieer, A. and Williams, R., 2010. Texture-based classification of sub-Antarctic vegetation communities on Heard Island. International Journal of Applied Earth Observation and Geoinformation, 12(3), pp.138-149.\nSun, L., Latifovic, R. and Pouliot, D., 2017. Haze removal based on a fully automated and improved haze optimized transformation for landsat imagery over land. Remote Sensing, 9(10), p.972.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>All about Remote Sensing Data</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "4  Earth Oberservation and Implications on Policy",
    "section": "4.4 References",
    "text": "4.4 References\nBai, H., Li, Z., Guo, H., Chen, H. and Luo, P., 2022. Urban green space planning based on remote sensing and geographic information systems. Remote Sensing, 14(17), p.4213.\nGov.sg, n.d. The Singapore Green Plan. Obtained from https://www.greenplan.gov.sg/\nKarteris, M., Theodoridou, I., Mallinis, G., Tsiros, E. and Karteris, A., 2016. Towards a green sustainable strategy for Mediterranean cities: Assessing the benefits of large-scale green roofs implementation in Thessaloniki, Northern Greece, using environmental modelling, GIS and very high spatial resolution remote sensing data. Renewable and Sustainable Energy Reviews, 58, pp.510-525.\nThe Straits Times, 2025. Heat-reflective paint initiative to be rolled out to all HDB estates by 2030. Obtained from https://www.straitstimes.com/singapore/housing/heat-reflective-paint-initiative-to-be-rolled-out-to-all-hdb-estates-by-2030",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Earth Oberservation and Implications on Policy</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5  Google Earth Engine",
    "section": "",
    "text": "5.1 What is Google Earth Engine?\nGoogle Earth Engine (GEE) is a cloud-based web application that can able to access large amounts of geospatial datasets and carry out analysis on these data without hosting these large datasets in local computers. It is designed not only for researchers, but for anyone to gain access to high-performance computing resources and share them easily through its own interactive application development tools.\nGEE is a game changer in terms of geospatial analysis because of its unique architecture - users interact with GEE through Javascript code editor and write scripts that define the data used and what analysis to carry out (client side), and the script is sent to Google’s cloud servers for processing (server side), before results are computed and visualization is displayed back on the code editor/maps on the client side.\nSome of the features of GEE can be found below\nThe power of GEE lies in being able to handle all the different functions across different datasets for multiple sensors across 40+ years, which opened up new possibilities in geospatial analysis such as real-time environmental monitoring. This has real policy impacts for governments as they are able to use GEE to set up and designate protection zones for example.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "chapter5.html#what-is-google-earth-engine",
    "href": "chapter5.html#what-is-google-earth-engine",
    "title": "5  Google Earth Engine",
    "section": "",
    "text": "Scale (Pixel Resolution)ProjectionFunctions and Tools\n\n\nThe resolution of images on GEE is set by output and not input; it automatically adjusts the resolution by using image pyramiding based on the closest scale used in the analysis and resamples as needed (default nearest neighbour method). This reduces memory and processor demands, leading to quicker visualisation. However, this can be a problem for users who wants to a specific resolution, in that case they would need to use reproject().\n\n\nGEE is also very straightforward when it comes to projection - it automatically converts to WGS84 for all display and visualisation so you don’t have to set any projections.\n\n\nGEE is a one-stop web application which can incorporate all steps in a geosptial analysis process including:\n\nReading in data (image collection) and filtering based on conditions (cloud coverage, time period, regions etc)\nOperations to manipulate the data including geometry operations, machine learning, deep learning etc\nHost outputs in terms of geospatial applications, dashboards, online charts\n\n\n\n\n\n\n\n\nFigure 1: Brazillian government uses GEE powered deforestion detection system called DETER-R which analyses Sentinel-1 images time series which is updated daily and signals warnings to help combat illegal logging and deforestation of the Amazon rainforest. Source: Doblas et al. (2022)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "chapter5.html#applications",
    "href": "chapter5.html#applications",
    "title": "5  Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nAs mentioned earlier, the introduction of GEE meant that large amount of remote sensing data across a long time period can now be accessed and analysed without downloading individual image into local computers, which enables researchers to focus on the front-end processing and analysis without worrying about the back-end data acquisition.\nOne of the biggest application of GEE is in the Land Use/Land Cover (LULC) field, where researchers can now examine LULC changes across a longer time period, which is incredibly important especially when analyzing processes that occur over time, such as urbanisation, hydrological processes, and climate change. GEE offers unprecedented coverage across multiple sources, time and geographical range which makes it suitable to analyse LULC changes and its impact. The capabilities of GEE is what made research done by Hao at al. (2019) possible, where they analysed the impact of the Three Gorges Reservoir Catchment area across 15 years based on land use changes and climate variation. The authors integrated the use of GEE with GlobeLand30, a dataset developed by China to achieve a full coverage of 15 years since GEE’s Landsat 8 data can fill in missing images from GlobeLand30’s dataset. The figure below shows how a typical GEE workflow might look like for a LULC analysis case.\n\n\n\nFigure 2: Work flow of analysing LULC by Hao et al. (2019).\n\n\nUsing an extended time period in their research, the authors are able to analyse differences in LULC, vegetation cover and climate and how the Three Gorges Dam project have caused extensive changes in the local region, which involve massive loss of grassland, cultivated land and forest in specific counties. But at the same time, the authors could also evaluate efforts by the government to restore vegetation in the area, identifying areas with improved LST. This highlights GEE’s advantages - real time continuous monitoring of resources at a global scale which is by far its most attractive feature.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "chapter5.html#reflections",
    "href": "chapter5.html#reflections",
    "title": "5  Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nI was quite excited to try out GEE this week as I was not aware of its existence when working with remote sensing data a few years back; it seems that this application is too good to be true - it has 40+ years of remote sensing data, no downloads required, and it even has APIs to enable users to build dashboards and interactive maps! (which is the goal of CASA0025 Building Spatial Applications)\nHowever, as I went through the practical, I realized that GEE is not a straightforward tool to use. Beside needing to use its Javascript code editor to run the script (although a python API is available as well), there are several restrictions with using GEE:\n\nImage analysis is limited to tools within the GEE API - several advanced atmospheric correction options are not available unless it is implemented manually; to develop new tools require understanding the back-end server side algorithms\nThere is an issue with data protection - although you can upload up to 250GB of user data, these data are stored in a private company (Google) servers which is against data sovereignty and security policies for some governments, which also limits its original purpose of making geospatial analysis accessible for many, including informing policies.\nData mining models for classification are also limited to SVM, CART and RF which may not be sufficient for current advancements in image classification such as Convolutional Neural Network/ Deep learning methods\n\nThese limitations add to the steep learning curve of using GEE, which I feel is still out of reach to many unless you are a developer or researcher. While it has made remote sensing data more accessible and is able to ingest large amounts of data and compute with way lesser time, to effectively utilise its capabilities require lots of time and effort trying to understand its algorithms and architecture.\nAnother important implication from the introduction of GEE is the potentially widening knowledge gap of using GEE between developed and developing countries. Despite GEE’s claims of accessibility for everyone, uptake of GEE in developing countries research has been slow due to lack of technical skill support, and more importantly, a lack of collaborations and lacking internet infrastructure (Vijayakumar, 2024). This is a real issue which requires targeted interventions to bridge the digital divide and ensure equitable access to geospatial technology.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "chapter5.html#references",
    "href": "chapter5.html#references",
    "title": "5  Google Earth Engine",
    "section": "5.4 References",
    "text": "5.4 References\nDoblas, J., Reis, M.S., Belluzzo, A.P., Quadros, C.B., Moraes, D.R., Almeida, C.A., Maurano, L.E., Carvalho, A.F., Sant’Anna, S.J. and Shimabukuro, Y.E., 2022. DETER-R: an operational near-real time tropical forest disturbance warning system based on Sentinel-1 time series analysis. Remote Sensing, 14(15), p.3658.\nHao, B., Ma, M., Li, S., Li, Q., Hao, D., Huang, J., Ge, Z., Yang, H. and Han, X., 2019. Land use change and climate variation in the three gorges reservoir catchment from 2000 to 2015 based on the Google Earth Engine. Sensors, 19(9), p.2118.\nVijayakumar, S., Saravanakumar, R., Arulanandam, M. and Ilakkiya, S., 2024. Google Earth Engine: empowering developing countries with large-scale geospatial data analysis—a comprehensive review. Arabian Journal of Geosciences, 17(4), p.139.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "6  Classification",
    "section": "",
    "text": "6.1 Summary of Classification\nThis week covers an fundamental yet complex concept in remote sensing - classification. Being an important part of remote sensing, image classification is a multi-step process which extracts features from images and sorting them into different distinct categories. These features can either be individual pixels or group of pixels, and are useful in understanding land use, urban planning, and other various uses. Image classification simplifies the raw spectral data into easily understood categories (like land use) which enables analysis of spatial patterns.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "chapter6.html#summary-of-classification",
    "href": "chapter6.html#summary-of-classification",
    "title": "6  Classification",
    "section": "",
    "text": "6.1.1 Types of Image Classifications\nThere are three main types of image classification:\n\nSupervised classification\nUnsupervised classification\nObject-Based classification (covered next week)\n\n\n\n\n\n\n\n\n\n\nType\nHow it works\nAdvantages/Disadvantages\nExample\n\n\n\n\nSupervised Classification\nThe user provides a set of training samples for different known categories of interest; the classifier algorithms learns from these training datasets and classifies the entire image\nHigher accuracy, higher user involvement in choosing training data, more time-consuming and more prone to overfitting, more control over classes (user defined)\nRandom Forest is a supervised classification algorithm which produces multiple decision trees, using a randomly subset of training samples and input features. The different trees contributes to the prediction of classes which prevents overfitting.\n\n\nUnsupervised Classification\nThe classification is carried out without prior knowledge or training samples; the clustering algorithm clusters pixels based on similar spectral values and the analyst then attempts to assign the spectral classes into thematic classes that is useful for analysis.\nLower accuracy, lower user involvement, less time-consuming and less prone to overfitting, less control (user determine cluster count)\nISODATA is a unsupervised algorithm which calculates class means in given dataset, then performing an iterative clustering procedure based on minimum-distance to reclassify pixels until input threshold parameters are met.\n\n\n\nSome of the questions we have to consider when choosing different image classification techniques:\n\nWhat is the data type that is available? What resolution is it? (drone, satellite, multi-spectral?)\nWhat do we want to classify? For thematic mapping or object-based tasks?\nIs labelled data available?\n\n(edit from week 8 - apparently the lack of SAR labelled data makes it difficult to be used for classification but there are still benefits to SAR data compared to multi-spectral data - see week 8)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "chapter6.html#applications",
    "href": "chapter6.html#applications",
    "title": "6  Classification",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nImage classification is an important methodology employed in vegetation studies as researchers are able to identify specific species of vegetation or structural types in a given area based on spectral signatures, and gain a better understanding of the spatial variations of vegetation covers (Zak and Cabido, 2002). This is crucial in efforts of nature conservation, agriculture monitoring and quatifiying ecosystem services.\nDeval and Joshi (2022) in their vegetation study of Keoladeo National Park, India, developed a detailed vegetation cover map as the area is facing several management challenges stemming from climate change:\n\nInvasive species such as water hyacinths are threatening the ecology within the wetland reserve and needs to be removed but the geographical extent of these species are not known\nLong term water supply is lacking and currently relies on periodic rainfall which is reducing in recent years\n\nImage classification would help build a comprehensive vegetation mapping of the National Park, building a species database for environmental conservation and management of the park. The authors used different image classification algorithms (Maximum Likelihood, Artificial Neural Network and Support Vector Machine) to classify vegetation and types of grasslands, each with their own strength in estimating different classes. The bootstrap method across 100 iterations is used to obtain overall accuracies, Kappa coefcient and respective width of confidence intervals for all three methods.\n\n\n\nFigure 1: Vegetation Types/Land Cover of Keoladeo National Park using MXL, ANN and SVM respectively. Source: Deval and Joshi (2022)\n\n\nThere are a few caveats of using image classification for such studies like vegetation identification though; Cingolani et al. (2004) highlights some issues when mapping vegetation using image classification techniques:\n\nLimited spatial resolution which means training sites of sufficient size might not be possible; this means smaller training sites must be used which increases variability in spectral signatures\nMismatch between discrete units chosen by researchers and what is identifiable on the satellite images\n\nThis reinforces the key questions before we engage in image classification algorithms including the type of data we have and what do we want to classify - this helps justify the choices we make for image classification and avoid spending time revisiting issues encountered downstream during the research.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "chapter6.html#reflections",
    "href": "chapter6.html#reflections",
    "title": "6  Classification",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nThis week covers the basics of classification, including the differences between supervised and unsupervised methods, its applications, and some more detailed explanation of machine learning algorithms like random forest and support vector machine. It took a bit to digest them especially the technical hyper-parameters gamma, and future reading on them made me a bit more uncertain about using an algorithm which I do not fully understand for remote sensing applications. For example, for random forest, we can choose max depth of trees, max no. of features, class weight and even quality of split. While using grid search can help us tune the hyperparameters, it is still important to understand what the parameters stand for instead of blindly tuning them which can result in overfitting due to certain parameters chosen or long computation time. Understanding the complexity of hyperparameters and how it influences classification in the area of interest can lead to improvement of methods like what Manfifard (2024) did - by introducing a new hyperparameter in modified Random Forest algorithm to better estimate crop yield instead of relying on traditional parameters.\nWith the introduction of LLMs like ChatGPT and cloud computing tools like GEE which can process large datasets quickly, it is easy to fall into a situation where we just use a range of tools (supervised, unsupervised, neural network, deepnet etc) to classify remote sensing data without fully understanding the underlying data characteristics, feature importance, or the appropriate model selection criteria. Accuracy is just one measure which does not mean much by itself - the interpretation of results in the context of study across different scales (spatial, temporal) and the implications of the findings is still the most important section of the research.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "chapter6.html#references",
    "href": "chapter6.html#references",
    "title": "6  Classification",
    "section": "6.4 References",
    "text": "6.4 References\nCingolani, A.M., Renison, D., Zak, M.R. and Cabido, M.R., 2004. Mapping vegetation in a heterogeneous mountain rangeland using Landsat data: an alternative method to define and classify land-cover units. Remote sensing of environment, 92(1), pp.84-97.\nDeval, K. and Joshi, P.K., 2022. Vegetation type and land cover mapping in a semi-arid heterogeneous forested wetland of India: Comparing image classification algorithms. Environment, Development and Sustainability, 24(3), pp.3947-3966.\nManafifard, M., 2024. A new hyperparameter to random forest: application of remote sensing in yield prediction. Earth Science Informatics, 17(1), pp.63-73.\nZak, M.R. and Cabido, M., 2002. Spatial patterns of the Chaco vegetation of central Argentina: Integration of remote sensing and phytosociology. Applied Vegetation Science, 5(2), pp.213-226.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "7  Classification II",
    "section": "",
    "text": "7.1 Classification II: OBIA, Sub-pixel Analysis and Accuracy\nThis week continues on last week’s content on image classification. The last type of image classification is Object-Based Image Analysis which focuses on identifying objects based on similarity or differences in pixels; a group of pixels that share common characteristics is called a superpixel, which is the output of many OBIA algorithms such as Simple Linear Iterative Clustering (SLIC).\nSub-pixel analysis however, looks at how a single pixel may represent different proportions of land use; this can be done using Spectral Mixture Analysis (SMA), based on the linear sum of endmembers weighted by the proportions of endmembers. This can yield meaningful results including super-resolution mapping, spectral unmixing or soft classification. However, sub-pixel analysis still suffers from several challenges which includes endmember selection problems, uncertainty and spectral mixing complexity amongst many others. Researchers has thus developed new algorithms and adopted other machine learning methods to help combat these issues.\nThe rest of the lecture covers the idea of accuracy of the image classification - which is usually seperated into:\nSome additional notes on accuracy:\nThe last part of the lecture covers cross validation",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "chapter7.html#classification-ii-obia-sub-pixel-analysis-and-accuracy",
    "href": "chapter7.html#classification-ii-obia-sub-pixel-analysis-and-accuracy",
    "title": "7  Classification II",
    "section": "",
    "text": "Producer Accuracy - correctly classified pixels to ground truth\nUser Accuracy - correctly classified pixels relative to all others classified as a particular land cover\nOverall Accuracy - combined fraction of correctly classified pixels across all land covers\n\n\n\nProducer and user accuracy are related to each other - it is not possible to get high accuracies in both\nThere is no ‘right’ choice in accuracy assessments - it depends on what the goal of the research is\nI previously used kappa in my own image classification project but realised now that it is not a good measure due to its arbitrary scale and how it varies based on levels of accuracy\nReceiver Operating Characteristic Curve is an alternative which covers the weaknesses of other accuracy measures by incorporating true negatives and the whole matrix, and enables comparison between different models\n\n\n\nBy partitioning the dataset into training and testing, we can measure the accuracy based on iterative partitioning; some examples include k-folds and Leave-One-Out cross validation methods\nSpatial autocorrelation also exists between training and testing because the training data can give a ‘sneak peak’ on the testing data if the spatial points are close to one another - this inflates the overall accuracy of the classifier\nSpatial cross validation can account for this issue through spatial partitioning of training/testing data; methods include spatial k-folds, spatial blocking, spatial buffer, or even a spatial Leave-One-Out cross validation\n\n\n\n\nFigure 1: Random (top) and spatial partitioning (bottom) of test and training data. Source: Lovelace et al., 2019",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "chapter7.html#applications",
    "href": "chapter7.html#applications",
    "title": "7  Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nWhile reading several papers on object based image classification (OBIA), I came across many papers that stated how pixel-based image classification is declining since 2010s because spatial resolution of images have improved, which shifts research towards OBIA. Even more recently, developments in deep learning and convolutional neural networks (CNN) produced prediction maps with improved accuracy. With higher resolution remote sensing imagery being commercially available from either satellite or drone sources, OBIA methods become increasingly important as they open up new applications and tools for monitoring and management of resources.\nFor example, novel use applications of OBIA and deep learning methods for agriculture weed mapping was used in UAV drone imagery in China by Huang et al. (2020) to spray site-specific pesticides which reduces costs of pesticides and creates less environmental harm from uniform spraying.\n\n\n\nFigure 2: Workflow of OBIA method in identifying weeds in rice fields. Source: Huang et al. 2020\n\n\nOne important learning that I picked from the paper is the importance of optimizing the hyper-parameters of the classifiers; the authors used a mixture of trial-and-error as well as existing research to support their choice of hyper-parameters. The authors also used context specific information (i.e looking for weeds) to help extracting useful information from the OBIA classification - for example shape and size was not a concern for them as weeds come in irregular sizes and shapes, but colour and textures are useful information for classification.\nWhile the authors managed to compare between using OBIA and deep learning methods for identifying weeds, the research was a technical paper that focused on the algorithms and specifics of identification, but there were no mentions of the weed spraying management plan, or how the new method would be applied in real life. It goes back the the class in week 4 where research should be linked to existing policies or real-life applications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "chapter7.html#reflections",
    "href": "chapter7.html#reflections",
    "title": "7  Classification II",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis week’s content (around 70 slides) were covered within a span of one hour, which is a lot of information and it took me a long time researching and reading to get a sense of what classification entails, how to measure accuracy and the current debates behind different measures of accuracy. The general idea I get from reading is that pixel-based analysis covered in the previous week is not as widely used and OBIA is hailed as a ‘superior’ method in current literature due to higher resolution images we are able to obtain from open source databases.\nThere is also seem to be a lot of development of using deep learning (DL) methods in remote sensing applications, with CNNs being used in OBIA and even sub-pixel analysis. However, with the introduction of these novel methods means that there is a need to revise traditional measures of accuracy in remote sensing. Maxwell et al. (2021) summarises some of the differences and similarities between traditional RS articles vs novel DL RS articles, where they concluded that newer DL RS papers have abandoned traditional RS accuracy terminologies, instead using computer science/AI aligned terminologies and accuracy assessment for their communication such as F1 scoring (recall/precision) and Intersection-Over-Union (IoU).\nWith the accelerating growth of AI in recent years, remote sensing is also impacted by the introduction of new algorithms, models and new methods of accuracy assessments are being introduced; I believe there is a greater need for the remote sensing community to standardise terminologies, notations and provide more details on the methodology (such as open-source codes).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "chapter7.html#references",
    "href": "chapter7.html#references",
    "title": "7  Classification II",
    "section": "7.4 References",
    "text": "7.4 References\nHuang, H., Lan, Y., Yang, A., Zhang, Y., Wen, S. and Deng, J., 2020. Deep learning versus Object-based Image Analysis (OBIA) in weed mapping of UAV imagery. International Journal of Remote Sensing, 41(9), pp.3446-3479.\nLovelace, R., Nowosad, J. and Muenchow, J., 2019. Geocomputation with R. Chapman and Hall/CRC.\nMaxwell, A.E., Warner, T.A. and Guillén, L.A., 2021. Accuracy assessment in convolutional neural network-based deep learning remote sensing studies—Part 1: Literature review. Remote Sensing, 13(13), p.2450.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Classification II</span>"
    ]
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "8  Synthetic Aperature Radar",
    "section": "",
    "text": "8.1 Synthetic Aperture Radar (SAR)\nThis final week of remote sensing covers more in-depth knowledge about SAR, especially its application in change detection. As mentioned in week 1, SAR is a type of active sensor which sends out radar signal and measures its return signal, also known as back-scattering.\nSome of the features of SAR include:\nIn my opinion, the high precision and resolution coverage of a large area is perhaps SAR’s biggest strength which made research into minute changes in landscape possible. Its weaknesses are seemingly being overcome with the advent of big data and deep learning which can correct errors in phase unwrapping and deformation (Li et al., 2022).\nThese SAR techniques would also have to evolve with the launch of more advanced SAR satellites capable of multi-polarisation, mutli-channel and multi-mode while current methods rely on singular polarisation data. It would be interesting to see how SAR technology will change in the future and how it can be combined with other technologies to develop new tools of measuring and analysing large scale phenomenons.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperature Radar</span>"
    ]
  },
  {
    "objectID": "chapter8.html#synthetic-aperture-radar-sar",
    "href": "chapter8.html#synthetic-aperture-radar-sar",
    "title": "8  Synthetic Aperature Radar",
    "section": "",
    "text": "Due to its long wavelength, different bands of SAR can penetrate through different mediums and therefore clouds is not a big issue as opposed to optical imaging. L-band signal for example, with a wavelength of about 23cm, can penetrate through a tree canopy and hence often used for ground monitoring.\nSurface Texture data is also available from measuring the backscatter - smooth surfaces result in small backscatter but rough surfaces scatter the signal in multiple directions, resulting in higher backscattering.\n\nThis can also be determined by how SAR sends and receive signals in different polarisation (HH, HV, VV and VH). The different polarisation are sensitive to different types of surfaces, giving rise to perceived surface textures like bare earth, vegetation and water.\n\nThe spatial resolution of radar data is related to the ratio of sensor wavelength to the radio antenna length. In simple terms, the longer the antenna, the higher the spatial resolution. Scientists developed the technique “synthetic aperture” which combines periodic electromagnetic signals into a virtual aperture that is much longer than the antenna width required.\nSAR sensors can also detect phase change - used to detect small ground movements; this is known as interferogram or InSAR\n\nIts original purpose is to obtain the surface elevation through different of two different InSAR images, but the field has broadened to new methods and techniques like D-InSAR and Small Baseline Subset (SBAS) InSAR. These methods are used in measuring ground displacement, natural disasters and most recently, effects of climate change like land subsidence and glacier movements over a large area which traditional geological surveys struggle to carry out.\n\n\n\n\n\n\nProposed deep learning method for InSAR phase unwrapping by Li et al. (2021)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperature Radar</span>"
    ]
  },
  {
    "objectID": "chapter8.html#applications",
    "href": "chapter8.html#applications",
    "title": "8  Synthetic Aperature Radar",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nDespite the strengths of SAR data which triumphs optical imagery due to its weather penetrating characteristic making it suitable for high-resolution, and near real-time change detection applications, the use of SAR data is not perfect. SAR images suffer from speckle noise caused by interference between different scatterers within a resolution cell. Layover and shadowing can also distort SAR imagery leading to misclassification of surface features like water or urban areas (Choi et al. 2022). Researchers has therefore tried to use multi-source remote sensing data to help validate SAR change detection.\nFor example, Hamidi et al. (2023) leveraged on both optical and SAR imagery to generate near real-time flood delineation maps for Hurricane Ida and Harvey. The flooded areas detected by SAR image changes were validated against multi-spectral imagery along with additional post processing like smoothing to reduce the speckle effect. This novel workflow hosted on Google Earth Engine is designed to overcome limitations of SAR imagery being susceptible to wind and heavy rain which can influence surface roughness of water, leading to incorrect flood identification. The research showed agreement percentages of 78-80% between SAR and optical imagery despite cloud interference for optical imagery, and proved to be a useful way to validate flood extent maps.\n\n\n\nFigure 2: Comparison of SAR, optical imagery (MNDWI), combination of both, as well as unique SAR flood extent captured under clouds. Source: Hamidi et al., 2023.\n\n\nIn the research, the multi-source remote sensing method truly shines in its disaster relief applications: while SAR data may miss some flood areas due to surface roughness interference from wind/heavy rain or being hidden under thick vegetation/built up areas (which causes issues with double-bounce backscattering), optical imagery can make up for missed areas since it is better to overestimate flooded areas than underestimate them (error of omission vs error of commision). This is also complemented by the capabilities of GEE which is able to run the script quickly to generate near real-time flood maps which is critical for disaster response and rescue efforts.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperature Radar</span>"
    ]
  },
  {
    "objectID": "chapter8.html#reflections",
    "href": "chapter8.html#reflections",
    "title": "8  Synthetic Aperature Radar",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\nThis week covers a seperate type of sensor that works fundamentally differently compared to the optical imagery and its associated methods covered in the past few weeks. I was left a bit confused - does this mean that the methods covered in previous weeks like supervised/unsupervised classification, sub-pixel analysis does not apply to SAR data since they are used for multi-spectral imagery? As I researched further, I realised that classification techniques can indeed be used on SAR data, but it is challenging because of the inherent speckle noise, often resulting in lower classification accuracy than optical remote sensing data (Mahdavi et a. 2017). However, many studies still use classification techniques on SAR data along with optical data because SAR data has a high revisit frequency and all-weather capturing ability, which is valuable for monitoring changes across time (Zhao et al. 2020).\nMy key understanding is that while some methods like classification is common through all types of remote sensing images, there are methods/processes specific to the characteristic of the data; for example an entire body of research dedicated to SAR data processing such as despeckling and unravelling deformation.\nThe introduction of high resolution SAR also does not take away the importance and capabilities of multi-spectral data; Thermal studies like urban heat island and air pollution studies can only be carried out through multispectral sensors. For now, SAR applications are seemingly limited to land mapping (LULC, forest monitoring etc), parameter retrieval (soil moisture etc) or object detection (ships, aircrafts etc) (Tsokas et al., 2022). I am excited to see what future applications of SAR could be - especially with the use of multi-polarisation SAR data, we might be able to better understand the scattering mechanisms across different materials. This will potentially open up new frontiers in new fields or disciplines such as archaeology, ethology (study of animal behaviour), or even wider aspects of climate change such as ocean health.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperature Radar</span>"
    ]
  },
  {
    "objectID": "chapter8.html#references",
    "href": "chapter8.html#references",
    "title": "8  Synthetic Aperature Radar",
    "section": "8.4 References",
    "text": "8.4 References\nChoi, J.H., Lee, M.J., Jeong, N.H., Lee, G. and Kim, K.T., 2022. Fusion of target and shadow regions for improved SAR ATR. IEEE Transactions on Geoscience and Remote Sensing, 60, pp.1-17.\nHamidi, E., Peter, B.G., Muñoz, D.F., Moftakhari, H. and Moradkhani, H., 2023. Fast flood extent monitoring with SAR change detection using google earth engine. IEEE Transactions on Geoscience and Remote Sensing, 61, pp.1-19.\nMahdavi, S., Maghsoudi, Y. and Amani, M., 2017. Effects of changing environmental conditions on synthetic aperture radar backscattering coefficient, scattering mechanisms, and class separability in a forest area. Journal of Applied Remote Sensing, 11(3), pp.036015-036015.\nLi, S., Xu, W. and Li, Z., 2022. Review of the SBAS InSAR Time-series algorithms, applications, and challenges. Geodesy and Geodynamics, 13(2), pp.114-126.\nLi, L., Zhang, H., Tang, Y., Wang, C. and Gu, F., 2021. InSAR phase unwrapping by deep learning based on gradient information fusion. IEEE Geoscience and Remote Sensing Letters, 19, pp.1-5.\nTsokas, A., Rysz, M., Pardalos, P.M. and Dipple, K., 2022. SAR data applications in earth observation: An overview. Expert Systems with Applications, 205, p.117342.\nZhao, W., Qu, Y., Chen, J. and Yuan, Z., 2020. Deeply synergistic optical and SAR time series for crop dynamic monitoring. Remote Sensing of Environment, 247, p.111952.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Synthetic Aperature Radar</span>"
    ]
  }
]